# ============================================================================ #
# ‚öôÔ∏è GENERAL CONFIGURATION FILE
# ---------------------------------------------------------------------------- #
# This file defines all key components for training and evaluating the SR model.
# Sections: Data, Model, Training, Architecture, Optimization, Scheduling, Logging
# ============================================================================ #


# ============================================================================ #
# üóÇÔ∏è DATA SETTINGS
# ---------------------------------------------------------------------------- #
Data:
  # Loader parameters
  train_batch_size: 12        # Batch size for training
  val_batch_size: 8           # Batch size for validation
  num_workers: 6              # Number of parallel workers for dataloader
  prefetch_factor: 2          # Samples prefetched per worker (2 is stable default)

  # Dataset configuration
  dataset_type: 'SISR_WW'       # Choose dataset type: ['cv', 'SPOT6', 'S2_6b', 'SISR_WW']
  normalization: 'normalise_10k' # Normalization strategy for data processing


# ============================================================================ #
# üß† MODEL AND CHECKPOINT SETTINGS
# ---------------------------------------------------------------------------- #
Model:
  in_bands: 4                 # Number of input channels (e.g. RGB-NIR-SWIR etc.)
  continue_training: False    # Resume full training (PL checkpoint path or False)
  load_checkpoint: False      # Load weights only (path or False)


# ============================================================================ #
# üèãÔ∏è TRAINING CONFIGURATION
# ---------------------------------------------------------------------------- #
Training:  
  # --- Hardware Setup
  gpus: [0,1,2,3]                    # Number of GPUs to use, individually in list form, e.g. [0] or [0,2]
  # --- General Training Setup
  max_epochs: 9999            # Maximum number of training epochs
  val_check_interval: 1.0     # Validate at x percent of epoch (float) or every N steps (int)
  limit_val_batches: 250      # Limit number of validation batches
  
  # --- Pretraining and adversarial setup ---
  pretrain_g_only: True        # Train generator only for initial phase
  g_pretrain_steps: 15000     # Number of generator-only warmup steps
  adv_loss_ramp_steps: 2500   # Gradual adversarial weight ramp steps
  label_smoothing: True        # Discriminator target smoothing (1.0 ‚Üí 0.9)

  EMA:
    enabled: True             # Maintain exponential moving average of generator weights
    decay: 0.999               # EMA decay factor (closer to 1.0 ‚Üí smoother updates)
    update_after_step: 0       # Delay EMA updates until this global step (0 = immediate)
    use_num_updates: True      # Use adaptive decay warmup based on number of updates

  Losses:
    # --- GAN term ---
    adv_loss_beta: 1e-3        # Final adversarial loss weight after ramp-up
    adv_loss_schedule: 'cosine'   # Adversarial weight ramp type: ['linear', 'cosine']

    # --- Content loss components (GeneratorContentLoss) ---
    l1_weight: 1.0             # L1 loss over all bands
    sam_weight: 0.05           # Spectral Angle Mapper loss
    perceptual_weight: 0.1     # Perceptual similarity term weight
    perceptual_metric: 'vgg'   # ['vgg', 'lpips'] - LPIPS requires pip install lpips
    tv_weight: 0.0             # Total Variation regularization (optional)

    # --- Metric evaluation settings ---
    max_val: 1.0               # Peak value assumed by PSNR/SSIM after metric preprocessing
    ssim_win: 11               # SSIM window size (must be odd integer)


# ============================================================================ #
# üß© ARCHITECTURAL PARAMETERS
# ---------------------------------------------------------------------------- #
# See Docs for archtecture details and suggestions
Generator:
  model_type: 'rcab'           # Block type: ['SRResNet', 'res', 'rcab', 'rrdb', 'lka', 'conditional_cgan'/'cgan']
  large_kernel_size: 9         # Kernel for head and tail conv layers
  small_kernel_size: 3         # Kernel for intermediate blocks
  n_channels: 96               # Number of feature channels (original 64)
  n_blocks: 16                 # Number of residual/attention blocks (original 16)
  scaling_factor: 4            # Upscaling factor (e.g., 2√ó, 4√ó, 8√ó)

Discriminator:
  model_type: 'standard'       # Discriminator architecture selector ['standard', 'patchgan']
  n_blocks: 8                  # Number of convolutional blocks / layers: ['standard': 8, 'patchgan': 3]

# ============================================================================ #
# üßÆ OPTIMIZATION SETTINGS
# ---------------------------------------------------------------------------- #
Optimizers:
  optim_g_lr: 1e-4             # Learning rate for Generator
  optim_d_lr: 1e-4             # Learning rate for Discriminator


# ============================================================================ #
# üìâ SCHEDULERS AND EARLY STOPPING
# ---------------------------------------------------------------------------- #
Schedulers:
  g_warmup_steps: 5000        # Generator warmup duration in steps (0 disables warmup)
  g_warmup_type: 'cosine'      # Generator warmup curve: ['cosine', 'linear']
  metric: 'val_metrics/l1'     # Metric monitored for both schedulers
  patience_g: 100              # Patience (epochs) for Generator LR scheduler
  patience_d: 100              # Patience (epochs) for Discriminator LR scheduler
  factor_g: 0.5                # LR reduction factor for Generator
  factor_d: 0.5                # LR reduction factor for Discriminator
  verbose: True                # Enable scheduler logging output


# ============================================================================ #
# üßæ LOGGING SETTINGS
# ---------------------------------------------------------------------------- #
Logging:
  num_val_images: 5            # Number of validation images logged per epoch
