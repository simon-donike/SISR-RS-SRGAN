# Getting started

This guide walks through installing dependencies, configuring datasets, and launching your first ESA OpenSR experiment. The stack
uses Python 3.10+, PyTorch Lightning, and Weights & Biases for experiment tracking.

> ðŸ’¡ **Only need inference?** Install the published package instead: `python -m pip install opensr-srgan`. It exposes
> `load_from_config` and `load_inference_model` so you can instantiate models without cloning the repository. Continue with the
> rest of this guide when you want to train, fine-tune, or otherwise modify the codebase.

## 1. Install the environment

1. **Create a virtual environment.**
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```
2. **Install Python dependencies.**
   ```bash
   pip install -r requirements.txt
   ```
3. **Authenticate logging backends (optional but recommended).**
   * Run `wandb login` to capture metrics and images in your W&B workspace.
   * Start `tensorboard --logdir logs/` if you prefer local dashboards.

## 2. Gather training data

ESA OpenSR currently ships with dataset wrappers for Sentinel-2 SAFE archives and the SEN2NAIP worldwide dataset.

* **Sentinel-2 SAFE (6-band or 4-band).** Build a manifest JSON that lists the LR/HR pairs generated by your tiling pipeline. The
  sample configuration expects a file similar to `/data3/S2_20m/s2_safe_manifest_20m.json`.
* **SEN2NAIP worldwide.** Point the configuration to the root directory that contains the preprocessed train/val splits.

Refer to [Data](data.md) for dataset-specific requirements and how to plug in new sources.

## 3. Configure the experiment

Use of the provided YAML presets or copy and edit one:

```bash
cp opensr_srgan/configs/config_10m.yaml opensr_srgan/configs/my_experiment.yaml
```

Update at least the following fields:

* `Data.dataset_type`: Choose `S2_6b`, `S2_4b`, or `SISR_WW`.
* `Generator.scaling_factor`: Set the desired upscaling (e.g., `4` or `8`).
* `Model.load_checkpoint`: Provide a path if you want to fine-tune an existing checkpoint.
* `Training.Losses.perceptual_metric`: Switch to `lpips` if you installed the optional dependency.

See [Configuration](configuration.md) for a full breakdown of available options.

## 4. Launch training

Run the training script with your customised config:

```bash
python -m opensr_srgan.train --config opensr_srgan/configs/my_experiment.yaml
```

The script will:

1. Instantiate the `SRGAN_model` Lightning module from the YAML file.
2. Build the appropriate dataset pair and wrap it in a `LightningDataModule`.
3. Configure Weights & Biases and TensorBoard loggers alongside checkpointing and learning-rate monitoring callbacks.
4. Start alternating generator/discriminator optimisation according to your warm-start schedule.

Training resumes automatically if `Model.continue_training` points to a Lightning checkpoint. If you interrupt training, always use the `Model.continue_training` flag to pass the generated checkpoint, since that restores all optimizers, schedulers, EMA etc.

## 5. Run validation or inference

* **Validation metrics** are logged at the end of each epoch, including L1, SAM, PSNR/SSIM (from the content loss helper), and
  discriminator statistics.
* **Qualitative monitoring** is available through Weights & Biases image panels when `Logging.num_val_images` is greater than zero.
* **Inference** on new low-resolution tiles can reuse the Lightning module.
  * **When working from the PyPI package:**
    ```python
    from opensr_srgan import load_from_config, load_inference_model

    # Option A â€“ bring your own config + checkpoint (local path or URL)
    custom_model = load_from_config(
        config_path="opensr_srgan/configs/config_10m.yaml",
        checkpoint_uri="https://example.com/checkpoints/srgan.ckpt",
        map_location="cuda",  # optional
    )

    # Option B â€“ grab the published RGB-NIR/SWIR presets from Hugging Face
    preset_model = load_inference_model("RGB-NIR", map_location="cpu")
    ```
  * **When working from source:**
    ```python
    from opensr_srgan.model.SRGAN import SRGAN_model

    model = SRGAN_model("your_config.yaml")
    model.load_from_checkpoint("path/to/checkpoint.ckpt")
    sr_tiles = model.predict_step(lr_tiles)
    ```
  In all cases the helpers automatically normalise Sentinel-2 ranges, apply histogram matching, and denormalise outputs for
  easier comparison with the source imagery.

## 6. Create Data Pipeline

* **SR Sen2 Tiles**: Use `opensr-utils` to crop, SR, patch, and overlap whole Sentinel-2 tiles. (Note: Currently only supports RGB-NIR.)

## 6. Next steps

* Explore alternative generator backbones such as RCAB or RRDB by changing `Generator.model_type`.
* Adjust adversarial warm-up with `Training.pretrain_g_only`, `g_pretrain_steps`, and `adv_loss_ramp_steps` if you observe
  instability.
* Integrate new datasets by extending the factory in `opensr_srgan/data/data_utils.py` and documenting them in [Data](data.md).
