@inproceedings{ledig2017photo,
  title={Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
  author={Ledig, Christian and others},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{wang2018esrgan,
  title={ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks},
  author={Wang, Xintao and others},
  booktitle={ECCV Workshops},
  year={2018}
}

@inproceedings{zhang2018perceptual,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Zhang, Richard and others},
  booktitle={CVPR},
  year={2018}}

@misc{dong2015imagesuperresolutionusingdeep,
      title={Image Super-Resolution Using Deep Convolutional Networks}, 
      author={Chao Dong and Chen Change Loy and Kaiming He and Xiaoou Tang},
      year={2015},
      eprint={1501.00092},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1501.00092}, 
}

@misc{goodfellow2014generativeadversarialnetworks,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1406.2661}, 
}

@misc{kim2016deeplyrecursiveconvolutionalnetworkimage,
      title={Deeply-Recursive Convolutional Network for Image Super-Resolution}, 
      author={Jiwon Kim and Jung Kwon Lee and Kyoung Mu Lee},
      year={2016},
      eprint={1511.04491},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1511.04491}, 
}

@ARTICLE{11159252,
  author={Donike, Simon and Aybar, Cesar and Contreras, Julio and Gómez-Chova, Luis},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Near-Infrared Band Synthesis From Earth Observation Imagery With Learned Location Embeddings and Task-Driven Loss Functions}, 
  year={2025},
  volume={18},
  number={},
  pages={24319-24330},
  keywords={Remote sensing;Context modeling;Adaptation models;Training;Image reconstruction;Data models;Translation;Superresolution;Vegetation mapping;Spatial resolution;Deep learning;generative AI;generative adversarial networks (GANs);near-infrared (NIR);remote sensing (RS)},
  doi={10.1109/JSTARS.2025.3608941}}

@misc{satlassuperres,
      title={Zooming Out on Zooming In: Advancing Super-Resolution for Remote Sensing}, 
      author={Piper Wolters and Favyen Bastani and Aniruddha Kembhavi},
      year={2023},
      eprint={2311.18082},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.18082}, 
}

@ARTICLE{10375518,
  author={Meng, Fanen and Wu, Sensen and Li, Yadong and Zhang, Zhe and Feng, Tian and Liu, Renyi and Du, Zhenhong},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Single Remote Sensing Image Super-Resolution via a Generative Adversarial Network With Stratified Dense Sampling and Chain Training}, 
  year={2024},
  volume={62},
  number={},
  pages={1-22},
  keywords={Superresolution;Remote sensing;Image reconstruction;Feature extraction;Training;Generative adversarial networks;Task analysis;Dense sampling;dual attention module;generative adversarial network;remote sensing (RS);super-resolution (SR)},
  doi={10.1109/TGRS.2023.3344112}}

@misc{su2024intriguingpropertycounterfactualexplanation,
      title={Intriguing Property and Counterfactual Explanation of GAN for Remote Sensing Image Generation}, 
      author={Xingzhe Su and Wenwen Qiang and Jie Hu and Fengge Wu and Changwen Zheng and Fuchun Sun},
      year={2024},
      eprint={2303.05240},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.05240}, 
}

@ARTICLE{9787539,
  author={Jia, Sen and Wang, Zhihao and Li, Qingquan and Jia, Xiuping and Xu, Meng},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Multiattention Generative Adversarial Network for Remote Sensing Image Super-Resolution}, 
  year={2022},
  volume={60},
  number={},
  pages={1-15},
  keywords={Remote sensing;Generators;Convolution;Generative adversarial networks;Task analysis;Spatial resolution;Interpolation;Generative adversarial network (GAN);remote sensing image;super-resolution (SR)},
  doi={10.1109/TGRS.2022.3180068}}


@Article{rs15205062,
AUTHOR = {Wang, Xuan and Sun, Lijun and Chehri, Abdellah and Song, Yongchao},
TITLE = {A Review of GAN-Based Super-Resolution Reconstruction for Optical Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {15},
YEAR = {2023},
NUMBER = {20},
ARTICLE-NUMBER = {5062},
URL = {https://www.mdpi.com/2072-4292/15/20/5062},
ISSN = {2072-4292},
ABSTRACT = {High-resolution images have a wide range of applications in image compression, remote sensing, medical imaging, public safety, and other fields. The primary objective of super-resolution reconstruction of images is to reconstruct a given low-resolution image into a corresponding high-resolution image by a specific algorithm. With the emergence and swift advancement of generative adversarial networks (GANs), image super-resolution reconstruction is experiencing a new era of progress. Unfortunately, there has been a lack of comprehensive efforts to bring together the advancements made in the field of super-resolution reconstruction using generative adversarial networks. Hence, this paper presents a comprehensive overview of the super-resolution image reconstruction technique that utilizes generative adversarial networks. Initially, we examine the operational principles of generative adversarial networks, followed by an overview of the relevant research and background information on reconstructing remote sensing images through super-resolution techniques. Next, we discuss significant research on generative adversarial networks in high-resolution image reconstruction. We cover various aspects, such as datasets, evaluation criteria, and conventional models used for image reconstruction. Subsequently, the super-resolution reconstruction models based on generative adversarial networks are categorized based on whether the kernel blurring function is recognized and utilized during training. We provide a brief overview of the utilization of generative adversarial network models in analyzing remote sensing imagery. In conclusion, we present a prospective analysis of forthcoming research directions pertaining to super-resolution reconstruction methods that rely on generative adversarial networks.},
DOI = {10.3390/rs15205062}
}

@InProceedings{p1,
author = {Bau, David and Zhu, Jun-Yan and Wulff, Jonas and Peebles, William and Strobelt, Hendrik and Zhou, Bolei and Torralba, Antonio},
title = {Seeing What a GAN Cannot Generate},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{p2,
 author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi and Chen, Xi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Techniques for Training GANs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf},
 volume = {29},
 year = {2016}
}

@misc{p3,
      title={GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks}, 
      author={Christopher Bowles and Liang Chen and Ricardo Guerrero and Paul Bentley and Roger Gunn and Alexander Hammers and David Alexander Dickie and Maria Valdés Hernández and Joanna Wardlaw and Daniel Rueckert},
      year={2018},
      eprint={1810.10863},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1810.10863}, 
}

@article{agriculture,
title = {Remote sensing for agricultural applications: A meta-review},
journal = {Remote Sensing of Environment},
volume = {236},
pages = {111402},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2019.111402},
url = {https://www.sciencedirect.com/science/article/pii/S0034425719304213},
author = {M. Weiss and F. Jacob and G. Duveiller},
keywords = {Review, Agriculture, Remote sensing, Crop, Traits, Radiative transfer model, Inversion, Machine learning, Deep learning, Assimilation, Land use, Land cover, Yield, Precision farming, Phenotyping, Ecosystem services},
abstract = {Agriculture provides humanity with food, fibers, fuel, and raw materials that are paramount for human livelihood. Today, this role must be satisfied within a context of environmental sustainability and climate change, combined with an unprecedented and still-expanding human population size, while maintaining the viability of agricultural activities to ensure both subsistence and livelihoods. Remote sensing has the capacity to assist the adaptive evolution of agricultural practices in order to face this major challenge, by providing repetitive information on crop status throughout the season at different scales and for different actors. We start this review by making an overview of the current remote sensing techniques relevant for the agricultural context. We present the agronomical variables and plant traits that can be estimated by remote sensing, and we describe the empirical and deterministic approaches to retrieve them. A second part of this review illustrates recent research developments that permit to strengthen applicative capabilities in remote sensing according to specific requirements for different types of stakeholders. Such agricultural applications include crop breeding, agricultural land use monitoring, crop yield forecasting, as well as ecosystem services in relation to soil and water resources or biodiversity loss. Finally, we provide a synthesis of the emerging opportunities that should strengthen the role of remote sensing in providing operational, efficient and long-term services for agricultural applications.}
}

@article{mapping,
  title={Land cover mapping using remote sensing data},
  author={Al-Doski, Jwan and Mansor, Shattri B and Ng, H and San, P and Khuzaimah, Z},
  journal={American Journal for Geographical Information Systems},
  volume={2020},
  pages={33--45},
  year={2020}
}


@Article{ecosysetm,
AUTHOR = {Li, Zhaoqin and Xu, Dandan and Guo, Xulin},
TITLE = {Remote Sensing of Ecosystem Health: Opportunities, Challenges, and Future Perspectives},
JOURNAL = {Sensors},
VOLUME = {14},
YEAR = {2014},
NUMBER = {11},
PAGES = {21117--21139},
URL = {https://www.mdpi.com/1424-8220/14/11/21117},
PubMedID = {25386759},
ISSN = {1424-8220},
ABSTRACT = {Maintaining a healthy ecosystem is essential for maximizing sustainable ecological services of the best quality to human beings. Ecological and conservation research has provided a strong scientific background on identifying ecological health indicators and correspondingly making effective conservation plans. At the same time, ecologists have asserted a strong need for spatially explicit and temporally effective ecosystem health assessments based on remote sensing data. Currently, remote sensing of ecosystem health is only based on one ecosystem attribute: vigor, organization, or resilience. However, an effective ecosystem health assessment should be a comprehensive and dynamic measurement of the three attributes. This paper reviews opportunities of remote sensing, including optical, radar, and LiDAR, for directly estimating indicators of the three ecosystem attributes, discusses the main challenges to develop a remote sensing-based spatially-explicit comprehensive ecosystem health system, and provides some future perspectives. The main challenges to develop a remote sensing-based spatially-explicit comprehensive ecosystem health system are: (1) scale issue; (2) transportability issue; (3) data availability; and (4) uncertainties in health indicators estimated from remote sensing data. However, the Radarsat-2 constellation, upcoming new optical sensors on Worldview-3 and Sentinel-2 satellites, and improved technologies for the acquisition and processing of hyperspectral, multi-angle optical, radar, and LiDAR data and multi-sensoral data fusion may partly address the current challenges.},
DOI = {10.3390/s141121117}
}

@article{disaster,
  title={Smart remote sensing network for disaster management: An overview},
  author={Ahmad, Rami},
  journal={Telecommunication Systems},
  volume={87},
  number={1},
  pages={213--237},
  year={2024},
  publisher={Springer}
}



@ARTICLE{osrtest,
  author={Aybar, Cesar and Montero, David and Donike, Simon and Kalaitzis, Freddie and Gómez-Chova, Luis},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={A Comprehensive Benchmark for Optical Remote Sensing Image Super-Resolution}, 
  year={2024},
  volume={21},
  number={},
  pages={1-5},
  keywords={Measurement;Remote sensing;Spatial resolution;Superresolution;Reflectivity;Protocols;Inspection;Benchmarking;datasets;deep learning;NAIP;Sentinel-2 (S2);SPOT;super-resolution (SR)},
  doi={10.1109/LGRS.2024.3401394}}

@misc{osrutils,
  author       = {Donike, Simon and contributors},
  title        = {OpenSR-Utils: Large-scale inference and data processing utilities for super-resolution of remote sensing imagery},
  year         = {2025},
  howpublished = {\url{https://github.com/ESAOpenSR/opensr-utils}},
  note         = {Accessed: 2025-10-14}
}

@misc{rcab,
      title={Residual Channel Attention Generative Adversarial Network for Image Super-Resolution and Noise Reduction}, 
      author={Jie Cai and Zibo Meng and Chiu Man Ho},
      year={2020},
      eprint={2004.13674},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2004.13674}, 
}

@misc{rrdb,
      title={Residual Dense Network for Image Super-Resolution}, 
      author={Yulun Zhang and Yapeng Tian and Yu Kong and Bineng Zhong and Yun Fu},
      year={2018},
      eprint={1802.08797},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1802.08797}, 
}

@misc{lka,
      title={Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation}, 
      author={Liam Chalcroft and Ruben Lourenço Pereira and Mikael Brudfors and Andrew S. Kayser and Mark D'Esposito and Cathy J. Price and Ioannis Pappas and John Ashburner},
      year={2023},
      eprint={2308.07251},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2308.07251}, 
}

@misc{patchgan,
      title={Patch-Based Image Inpainting with Generative Adversarial Networks}, 
      author={Ugur Demir and Gozde Unal},
      year={2018},
      eprint={1803.07422},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1803.07422}, 
}

@misc{cyclegan,
      title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}, 
      author={Jun-Yan Zhu and Taesung Park and Phillip Isola and Alexei A. Efros},
      year={2020},
      eprint={1703.10593},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1703.10593}, 
}

@misc{px2px,
      title={Image-to-Image Translation with Conditional Adversarial Networks}, 
      author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
      year={2018},
      eprint={1611.07004},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.07004}, 
}

@misc{ema,
      title={Exponential Moving Average of Weights in Deep Learning: Dynamics and Benefits}, 
      author={Daniel Morales-Brotons and Thijs Vogels and Hadrien Hendrikx},
      year={2024},
      eprint={2411.18704},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.18704}, 
}
@article{ema2,
author = {Xi, Xiuliang and Jin, Xin and Jiang, Qian and Lin, Yu and Zhou, Wei and Guo, Lei},
title = {EMA-GAN: A Generative Adversarial Network for Infrared and Visible Image Fusion with Multiscale Attention Network and Expectation Maximization Algorithm},
journal = {Advanced Intelligent Systems},
volume = {5},
number = {11},
pages = {2300310},
keywords = {attention mechanism, expectation-maximization algorithm, generative adversarial networks, image fusion, infrared and visible image fusion},
doi = {https://doi.org/10.1002/aisy.202300310},
url = {https://advanced.onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202300310},
eprint = {https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.202300310},
abstract = {The purpose of the infrared and visible image fusion is to generate a fused image with rich information. Although most fusion methods can achieve good performance, there are still shortcomings in extracting feature information from source images, which make it difficult to balance the thermal radiation region information and texture detail information in the fused image. To address the above issues, an expectation maximization (EM) learning framework based on adversarial generative networks (GAN) for infrared and visible image fusion is proposed. The EM algorithm (EMA) can obtain maximum likelihood estimation for problems with potential variables, which is helpful in solving the problem of lack of labels in infrared and visible image fusion. The axial-corner attention mechanism is designed to capture long-range semantic information and texture information of the visible image. The multifrequency attention mechanism digs the relationships between features at different scales to highlight target information of infrared images in the fused result. Meanwhile, two discriminators are used to balance two different features, and a new loss function is designed to maximize the likelihood estimate of the data with soft class label assignments, which is obtained from the expectation network. Extensive experiments demonstrate the superiority of EMA-GAN over the state-of-the-art.},
year = {2023}
}



@misc{g1,
      title={Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?}, 
      author={Daniel Panangian and Ksenia Bittner},
      year={2025},
      eprint={2501.15847},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.15847}, 
}

@misc{allen,
      title={Zooming Out on Zooming In: Advancing Super-Resolution for Remote Sensing}, 
      author={Piper Wolters and Favyen Bastani and Aniruddha Kembhavi},
      year={2023},
      eprint={2311.18082},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.18082}, 
}

@ARTICLE{s1,
  author={Xiao, Yi and Yuan, Qiangqiang and Jiang, Kui and He, Jiang and Jin, Xianyu and Zhang, Liangpei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution}, 
  year={2024},
  volume={62},
  number={},
  pages={1-14},
  keywords={Remote sensing;Task analysis;Diffusion processes;Superresolution;Predictive models;Image reconstruction;Training;Diffusion probabilistic model (DPM);image super-resolution (SR);prior enhancement;remote sensing},
  doi={10.1109/TGRS.2023.3341437}}


@Article{s2,
AUTHOR = {Liu, Jinzhe and Yuan, Zhiqiang and Pan, Zhaoying and Fu, Yiqun and Liu, Li and Lu, Bin},
TITLE = {Diffusion Model with Detail Complement for Super-Resolution of Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {19},
ARTICLE-NUMBER = {4834},
URL = {https://www.mdpi.com/2072-4292/14/19/4834},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing super-resolution (RSSR) aims to improve remote sensing (RS) image resolution while providing finer spatial details, which is of great significance for high-quality RS image interpretation. The traditional RSSR is based on the optimization method, which pays insufficient attention to small targets and lacks the ability of model understanding and detail supplement. To alleviate the above problems, we propose the generative Diffusion Model with Detail Complement (DMDC) for RS super-resolution. Firstly, unlike traditional optimization models with insufficient image understanding, we introduce the diffusion model as a generation model into RSSR tasks and regard low-resolution images as condition information to guide image generation. Next, considering that generative models may not be able to accurately recover specific small objects and complex scenes, we propose the detail supplement task to improve the recovery ability of DMDC. Finally, the strong diversity of the diffusion model makes it possibly inappropriate in RSSR, for this purpose, we come up with joint pixel constraint loss and denoise loss to optimize the direction of inverse diffusion. The extensive qualitative and quantitative experiments demonstrate the superiority of our method in RSSR with small and dense targets. Moreover, the results from direct transfer to different datasets also prove the superior generalization ability of DMDC.},
DOI = {10.3390/rs14194834}
}

@ARTICLE{s3,
  author={Donike, Simon and Aybar, Cesar and Gómez-Chova, Luis and Kalaitzis, Freddie},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Trustworthy Super-Resolution of Multispectral Sentinel-2 Imagery With Latent Diffusion}, 
  year={2025},
  volume={18},
  number={},
  pages={6940-6952},
  keywords={Superresolution;Remote sensing;Training;Diffusion models;Measurement;Spatial resolution;Image reconstruction;Uncertainty;Adaptation models;European Space Agency;Deep learning;latent diffusion;model uncertainty;remote sensing (RS);Sentinel-2;super-resolution (SR)},
  doi={10.1109/JSTARS.2025.3542220}}


@misc{cgan,
      title={Information-theoretic stochastic contrastive conditional GAN: InfoSCC-GAN}, 
      author={Vitaliy Kinakh and Mariia Drozdova and Guillaume Quétant and Tobias Golling and Slava Voloshynovskiy},
      year={2021},
      eprint={2112.09653},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.09653}, 
}


@article{sen2naip,
  author    = {Aybar, Cesar and Montero, David and Contreras, Julio and Donike, Simon and Kalaitzis, Freddie and Gómez-Chova, Luis},
  title     = {SEN2NAIP: A large-scale dataset for Sentinel-2 Image Super-Resolution},
  journal   = {Scientific Data},
  year      = {2024},
  volume    = {11},
  number    = {1},
  pages     = {1389},
  doi       = {10.1038/s41597-024-04214-y},
  url       = {https://doi.org/10.1038/s41597-024-04214-y},
  abstract  = {The increasing demand for high spatial resolution in remote sensing has underscored the need for super-resolution (SR) algorithms that can upscale low-resolution (LR) images to high-resolution (HR) ones. To address this, we present SEN2NAIP, a novel and extensive dataset explicitly developed to support SR model training. SEN2NAIP comprises two main components. The first is a set of 2,851 LR-HR image pairs, each covering 1.46 square kilometers. These pairs are produced using LR images from Sentinel-2 (S2) and corresponding HR images from the National Agriculture Imagery Program (NAIP). Using this cross-sensor dataset, we developed a degradation model capable of converting NAIP images to match the characteristics of S2 imagery ($S_{2-like}$). This led to the creation of a second subset, consisting of 35,314 NAIP images and their corresponding $S_{2-like}$ counterparts, generated using the degradation model. With the SEN2NAIP dataset, we aim to provide a valuable resource that facilitates the exploration of new techniques for enhancing the spatial resolution of Sentinel-2 imagery.},
  issn      = {2052-4463}
}