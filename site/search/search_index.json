{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Remote-Sensing SRGAN","text":"<p>Remote-Sensing-SRGAN is a research-grade training stack for single-image super-resolution (SISR) of multispectral satellite imagery. It wraps a flexible generator/discriminator design, configurable loss suite, and remote-sensing specific data pipelines behind a configuration-first workflow. The implementation is optimised for Sentinel-2 but can be adapted to other sensors that provide paired low-/high-resolution observations.</p> <p>Why another SRGAN?</p> <p>Training GANs on multi-band Earth observation data is notoriously brittle. This project codifies the training heuristics that have proven stable in production \u2014 generator pretraining, adversarial weight ramp-up, and configurable discriminator cadence \u2014 while exposing every knob through YAML. The goal is to make it easy to reproduce remote-sensing SR experiments without rewriting boilerplate.</p>"},{"location":"#project-highlights","title":"Project highlights","text":""},{"location":"#flexible-generator-zoo","title":"Flexible generator zoo","text":"<p>Choose between SRResNet, residual, RCAB, RRDB, large-kernel attention, or conditional GAN backbones with scale factors from 2\u00d7\u20138\u00d7.</p>"},{"location":"#pluggable-losses","title":"Pluggable losses","text":"<p>Pixel, spectral, perceptual, adversarial, and total-variation terms can be mixed with independent weights and activation schedules.</p>"},{"location":"#remote-sensing-ready-datasets","title":"Remote-sensing ready datasets","text":"<p>Sentinel-2 SAFE windowing and the SEN2NAIP worldwide pairs are built in, with Lightning datamodules created on the fly from the configuration.</p>"},{"location":"#stabilised-training-flow","title":"Stabilised training flow","text":"<p>Generator-only warm-up, adversarial ramp-up, discriminator scheduling, and Lightning callbacks are wired into the training script.</p>"},{"location":"#comprehensive-logging","title":"Comprehensive logging","text":"<p>TensorBoard visualisations, Weights &amp; Biases tracking, and qualitative inspection panels are emitted during training.</p>"},{"location":"#repository-layout","title":"Repository layout","text":"<pre><code>SISR-RS-SRGAN/\n\u251c\u2500\u2500 configs/              # YAML experiment definitions\n\u251c\u2500\u2500 data/                 # Dataset implementations and helpers\n\u251c\u2500\u2500 model/                # LightningModule, generators, discriminators, losses\n\u251c\u2500\u2500 utils/                # Logging and spectral utilities\n\u251c\u2500\u2500 train.py              # Training entry point\n\u2514\u2500\u2500 docs/                 # MkDocs site (you are here)\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Head to Getting Started for environment setup and the minimal training command.</li> <li>Review the Configuration Guide to understand every YAML switch.</li> <li>Dive into Model Components for generator and discriminator details.</li> <li>Explore Data Pipelines for dataset specifics and extension tips.</li> </ul>"},{"location":"architecture/","title":"Model Components","text":"<p>Remote-Sensing-SRGAN builds on a modular Lightning implementation that lets you swap generator and discriminator backbones as well as plug in new loss terms. This section describes the main components and how they interact during training.</p>"},{"location":"architecture/#lightning-module","title":"Lightning module","text":"<p>The central <code>SRGAN_model</code> class orchestrates the entire training loop.</p>"},{"location":"architecture/#initialisation","title":"Initialisation","text":"<p>Configuration values are loaded through OmegaConf, losses are configured, and a model summary is printed for quick inspection.</p>"},{"location":"architecture/#model-wiring","title":"Model wiring","text":"<p><code>get_models()</code> attaches the generator and discriminator variants requested in the YAML configuration.</p> <pre><code>\n</code></pre>"},{"location":"architecture/#inference-helpers","title":"Inference helpers","text":"<p><code>forward</code> and <code>predict_step</code> wrap generator inference with automatic normalisation and histogram matching so predictions align with Sentinel-2 statistics.</p> <pre><code>\n</code></pre>"},{"location":"architecture/#training-hooks","title":"Training hooks","text":"<p>Lightning hooks drive generator-only pretraining, adversarial ramp-up, and detailed metric logging.</p>"},{"location":"architecture/#generator-zoo","title":"Generator zoo","text":"<p>Pick the generator backbone by setting <code>Generator.model_type</code> in the config. The options map to classes under <code>model/generators/</code>:</p> Type Description <code>SRResNet</code> Classic SRResNet with residual blocks sans batch norm; a strong baseline for content pretraining. <code>res</code> Flexible residual blocks with configurable depth/width defined in <code>FlexibleGenerator</code> (supports residual scaling). <code>rcab</code> Residual channel attention blocks for finer texture modelling using the same flexible generator with RCAB building blocks. <code>rrdb</code> Residual-in-residual dense blocks (ESRGAN-style) for deeper receptive fields, enabled through the flexible generator registry. <code>lka</code> Large kernel attention variant approximating global receptive fields, useful for broad RS structures. <code>conditional_cgan</code> / <code>cgan</code> Conditional GAN generator that injects latent noise and conditional embeddings in addition to the spectral input. <p>Each generator consumes <code>Model.in_bands</code> channels, expands to <code>Generator.n_channels</code> features, and upsamples by <code>Generator.scaling_factor</code> using pixel shuffle stages. Kernel sizes (<code>large_kernel_size</code>, <code>small_kernel_size</code>) tailor the receptive field for remote-sensing textures.</p> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>Specialised blocks (RRDB, RCAB, large-kernel attention) are registered in <code>model/model_blocks</code> and consumed by the flexible generator.</p> <pre><code>\n</code></pre> <p>Conditional GAN support routes through the dedicated generator class, which injects noise vectors and optional conditional embeddings.</p> <pre><code>\n</code></pre>"},{"location":"architecture/#discriminators","title":"Discriminators","text":"<p>Discriminator selection lives under <code>Discriminator.model_type</code>. The Lightning module wires either the standard SRGAN CNN or a PatchGAN variant and forwards multi-band inputs without assuming RGB ordering.</p> <pre><code>\n</code></pre> <p>The PatchGAN implementation and supporting building blocks live under <code>model/descriminators/</code>.</p> <pre><code>\n</code></pre> <p>Because both discriminators receive multi-band inputs, the repo avoids hard-coding RGB assumptions and allows training on arbitrary spectral stacks.</p>"},{"location":"architecture/#losses","title":"Losses","text":"<p>Generator training minimises a combination of content and adversarial losses. The Lightning module instantiates both criteria and scales them according to the YAML configuration.</p> <p><code>GeneratorContentLoss</code> mixes L1, spectral angle mapper, perceptual metrics, and total variation into a single callable.</p> <pre><code>\n</code></pre> <p>Training warm-ups and adversarial ramp schedules are defined directly in the configuration.</p> <pre><code>\n</code></pre>"},{"location":"architecture/#normalisation-and-inference-helpers","title":"Normalisation and inference helpers","text":"<p>Remote-sensing imagery often arrives as 0\u201310,000 scaled reflectance. <code>utils.spectral_helpers</code> provides <code>normalise_10k</code> for scaling to 0\u20131 and <code>histogram</code> for histogram matching. <code>predict_step</code> calls both utilities to ensure outputs match the statistical distribution of inputs before returning CPU tensors.</p> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"architecture/#logging-utilities","title":"Logging utilities","text":"<p>Qualitative logging is handled via <code>utils.logging_helpers.plot_tensors</code>, which renders low-/super-/high-resolution panels for TensorBoard and Weights &amp; Biases. The Lightning module invokes these helpers inside validation hooks so you can monitor spectral fidelity and artefacts during training.</p> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"architecture/#extending-the-model","title":"Extending the model","text":"<ul> <li>Add new generator families by creating a file under <code>model/generators/</code> and wiring it into <code>SRGAN_model.get_models</code> plus <code>model/generators/__init__.py</code>.</li> <li>Introduce alternative discriminators under <code>model/descriminators/</code> and add a new branch in <code>get_models</code>.</li> <li>Extend content loss by editing <code>model/loss/loss.py</code>; the <code>GeneratorContentLoss</code> class already parses weights from the config.</li> <li>For additional logging (e.g., metrics beyond PSNR/SSIM/LPIPS), modify the validation hooks inside <code>model/SRGAN.py</code>.</li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>Remote-Sensing-SRGAN is intentionally configuration-first. Every YAML file under <code>configs/</code> controls the entire experiment lifecycle \u2014 from dataset selection to optimiser schedules. This page documents the key sections and explains how they map to the Python implementation.</p>"},{"location":"configuration/#anatomy-of-a-config-file","title":"Anatomy of a config file","text":"<p>Each config follows the same structure as <code>configs/config_10m.yaml</code>:</p> <pre><code>\n</code></pre>"},{"location":"configuration/#data-block","title":"Data block","text":"<p>The data section controls loader throughput and selects the dataset implementation. The <code>dataset_type</code> key is forwarded to <code>data.data_utils.select_dataset</code>, which instantiates the proper dataset class and wraps it into a Lightning <code>DataModule</code> with the requested batch sizes and worker settings.</p> <pre><code>\n</code></pre>"},{"location":"configuration/#available-dataset-types","title":"Available dataset types","text":"Key Description <code>S2_6b</code> Loads Sentinel-2 SAFE chips with six 20\u202fm bands (B05, B06, B07, B8A, B11, B12). Histogram-matched low-resolution inputs are synthesised on-the-fly. <code>S2_4b</code> Reuses the SAFE pipeline for four 10\u202fm bands (B05, B04, B03, B02). Useful for RGB+NIR 4\u00d7 tasks. <code>SISR_WW</code> Wraps the SEN2NAIP worldwide dataset for 4\u00d7 cross-sensor training. Training/validation splits are constructed from a root directory. <p>If you introduce a new dataset class, register it by adding another branch to <code>select_dataset</code> and exposing a new <code>dataset_type</code> value.</p>"},{"location":"configuration/#model-block","title":"Model block","text":"<p>The <code>Model</code> section captures properties that affect both training and checkpoint management:</p> <ul> <li><code>in_bands</code>: number of channels consumed by the generator and discriminator. This value is forwarded directly when models are instantiated.</li> <li><code>load_checkpoint</code>: path to a Lightning checkpoint whose weights should initialise the model before training begins.</li> <li><code>continue_training</code>: checkpoint to resume including optimiser states and scheduler counters (mapped to <code>Trainer(resume_from_checkpoint=...)</code>).</li> </ul> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"configuration/#training-block","title":"Training block","text":"<p>Training-related switches are consumed inside the Lightning module:</p> <ul> <li><code>pretrain_g_only</code>: number of initial steps where only generator updates run (adversarial term disabled).</li> <li><code>g_pretrain_steps</code>: length of the generator-only warm-up window.</li> <li><code>adv_loss_ramp_steps</code>: number of iterations over which the adversarial loss weight ramps to its target. The schedule shape is controlled by <code>Losses.adv_loss_schedule</code>.</li> <li><code>label_smoothing</code>: enables 0.9 real labels in the discriminator to reduce overconfidence.</li> </ul> <pre><code>\n</code></pre> <p>The nested <code>Losses</code> dictionary is passed to <code>GeneratorContentLoss</code>, which mixes pixel-space (<code>l1_weight</code>), spectral (<code>sam_weight</code>), perceptual (<code>perceptual_weight</code> with <code>perceptual_metric</code>), and total-variation (<code>tv_weight</code>) losses. The final adversarial weight after ramp-up is <code>adv_loss_beta</code>.</p> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"configuration/#generator-and-discriminator-blocks","title":"Generator and Discriminator blocks","text":"<p>Generator parameters control the backbone constructed in <code>SRGAN_model.get_models</code>:</p> <ul> <li><code>model_type</code>: choose among <code>SRResNet</code>, <code>res</code>, <code>rcab</code>, <code>rrdb</code>, <code>lka</code>, or conditional GAN variants (<code>conditional_cgan</code>/<code>cgan</code>). Each path maps to a dedicated class under <code>model/generators/</code>.</li> <li><code>n_channels</code>, <code>n_blocks</code>: width and depth of the residual trunk. These values are forwarded verbatim to the generator constructors.</li> <li><code>scaling_factor</code>: upsampling factor (2\u00d7, 4\u00d7, or 8\u00d7). Passed into the generator to configure pixel-shuffle stages.</li> <li><code>large_kernel_size</code>, <code>small_kernel_size</code>: head/tail and residual block kernel sizes to shape receptive field.</li> </ul> <pre><code>\n</code></pre> <p>The discriminator section selects either the classic SRGAN CNN (<code>standard</code>) or a PatchGAN variant and optionally specifies convolutional depth via <code>n_blocks</code>.</p> <pre><code>\n</code></pre>"},{"location":"configuration/#optimisers-and-schedulers","title":"Optimisers and schedulers","text":"<p>Both the generator and discriminator use Adam optimisers with learning rates read from <code>Optimizers.optim_g_lr</code> and <code>Optimizers.optim_d_lr</code>. The Lightning module instantiates <code>ReduceLROnPlateau</code> schedulers using the parameters provided under <code>Schedulers</code> (<code>metric</code>, <code>patience_*</code>, <code>factor_*</code>, <code>verbose</code>).</p> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"configuration/#logging","title":"Logging","text":"<p>The <code>Logging</code> section controls qualitative outputs. During validation the model logs <code>num_val_images</code> panels via TensorBoard and Weights &amp; Biases. The training script also wires in Weights &amp; Biases, TensorBoard, and learning-rate monitors; edit this block if you want fewer images per epoch.</p> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"configuration/#tips-for-custom-configs","title":"Tips for custom configs","text":"<ul> <li>Keep configs under version control alongside experiment results \u2014 MkDocs can render them for quick reference.</li> <li>Use OmegaConf variable interpolation if you need to reuse values across sections.</li> <li>When experimenting with new datasets, add dataset-specific parameters (paths, band lists) under <code>Data</code> and consume them inside your dataset branch.</li> <li>Prefer creating new YAML files rather than editing defaults; the training script accepts any path via <code>--config</code>.</li> </ul>"},{"location":"data/","title":"Data Pipelines","text":"<p>Remote-Sensing-SRGAN ships with dataset loaders tailored to Sentinel-2 SAFE products and the SEN2NAIP worldwide corpus. All loaders produce <code>(lr, hr)</code> tensor pairs suitable for Lightning training and plug into the config-driven selector in <code>data/data_utils.py</code>.</p>"},{"location":"data/#dataset-selector","title":"Dataset selector","text":"<p><code>data.data_utils.select_dataset(config)</code> reads <code>config.Data.dataset_type</code> and constructs the appropriate dataset instances. The helper then wraps them into a minimal Lightning <code>DataModule</code> with configurable batch sizes, worker counts, and prefetch factors.</p> <pre><code>\n</code></pre> <pre><code>pl_datamodule = select_dataset(config)\ntrain_loader = pl_datamodule.train_dataloader()\nval_loader = pl_datamodule.val_dataloader()\n</code></pre> <p>The selector currently supports three dataset families:</p>"},{"location":"data/#sentinel-2-safe-6-band-s2_6b","title":"Sentinel-2 SAFE 6-band (<code>S2_6b</code>)","text":"<ul> <li>Goal: Train on six 20\u202fm Sentinel-2 bands (B05, B06, B07, B8A, B11, B12) stacked into a single tensor.</li> <li>Implementation: Uses <code>data/SEN2_SAFE/S2_6b_ds.py::S2SAFEDataset</code> to read a manifest of pre-windowed chips. The dataset handles normalisation, band ordering, LR synthesis via anti-aliased downsampling, and invalid chip filtering.</li> </ul> <p><pre><code>\n</code></pre> * Configuration hooks: <code>Generator.scaling_factor</code> determines the SR scale (2\u00d7/4\u00d7/8\u00d7). Hard-coded manifest path and band order can be promoted to config keys if you need to vary them frequently.</p>"},{"location":"data/#sentinel-2-safe-4-band-s2_4b","title":"Sentinel-2 SAFE 4-band (<code>S2_4b</code>)","text":"<ul> <li>Goal: Focus on RGB+NIR super-resolution using four 10\u202fm bands (B05, B04, B03, B02).</li> <li>Implementation: Reuses <code>S2SAFEDataset</code> with an alternative band list. Despite the 10\u202fm intent, the manifest path currently points to the 20\u202fm JSON; adjust it if you curate a dedicated 10\u202fm manifest.</li> </ul> <p><pre><code>\n</code></pre> * Configuration hooks: Same as <code>S2_6b</code>; update band order and manifest in the dataset branch if you require different inputs.</p>"},{"location":"data/#sen2naip-worldwide-sisr_ww","title":"SEN2NAIP Worldwide (<code>SISR_WW</code>)","text":"<ul> <li>Goal: Leverage Taco Foundation\u2019s SEN2NAIP pairs where Sentinel-2 observations are paired with NAIP aerial imagery at 4\u00d7 resolution.</li> <li>Implementation: The <code>SISRWorldWide</code> dataset (under <code>data/SISR_WW/</code>) reads the dataset root, honours <code>split</code> arguments (<code>train</code>/<code>val</code>), and returns aligned <code>(lr, hr)</code> samples. The data selector simply instantiates train/val splits pointing to <code>/data3/SEN2NAIP_global</code> by default.</li> </ul> <p><pre><code>\n</code></pre> * Configuration hooks: Update the root path or expose it through the config to point at your local dataset copy.</p>"},{"location":"data/#building-new-datasets","title":"Building new datasets","text":"<p>Adding a dataset follows three simple steps:</p> <ol> <li>Implement the dataset class under <code>data/&lt;name&gt;/</code> with <code>__len__</code> and <code>__getitem__</code> returning <code>(lr, hr)</code> arrays or tensors.</li> <li>Register the selector by adding a new <code>elif</code> branch in <code>select_dataset</code> that instantiates your dataset and passes the config values you need.</li> <li>Expose configuration keys under <code>Data</code> (e.g., <code>manifest_path</code>, <code>bands_keep</code>) so experiments remain reproducible without code edits.</li> </ol> <pre><code>\n</code></pre>"},{"location":"data/#dataloader-configuration","title":"DataLoader configuration","text":"<p><code>datamodule_from_datasets</code> handles DataLoader construction and mirrors config values to PyTorch Lightning:</p> <ul> <li><code>train_batch_size</code> / <code>val_batch_size</code>: separate sizes for each split, falling back to <code>Data.batch_size</code> if provided.</li> <li><code>num_workers</code>: enables multi-process loading; when set to zero the helper disables persistent workers and prefetch factors.</li> <li><code>prefetch_factor</code>: forwarded when workers &gt; 0 to balance host-device throughput.</li> <li><code>shuffle</code>: enabled for both train and validation to improve spatial diversity in evaluation batches (intentional design choice).</li> </ul> <pre><code>\n</code></pre> <p>Because the <code>DataModule</code> is assembled dynamically, you can plug in custom datasets without changing the training loop \u2014 just add a branch and update your YAML.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks through environment setup, configuration management, and the minimal commands required to launch a Remote-Sensing-SRGAN experiment.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>The project pins PyTorch 1.13 and torchvision 0.14 wheels that target Python 3.10. Set up your environment with that interpreter and install dependencies before launching training.</p> <p>The default trainer is configured for CUDA acceleration and streams metrics to both TensorBoard and Weights &amp; Biases.</p> <p>Create a Weights &amp; Biases account if you plan to use the built-in experiment tracking, and install Git LFS when working with large dataset manifests.</p>"},{"location":"getting-started/#create-an-environment","title":"Create an environment","text":"<pre><code># Clone the repository\ngit clone https://github.com/ESAOpenSR/Remote-Sensing-SRGAN.git\ncd Remote-Sensing-SRGAN\n\n# (optional) Create a Python 3.10 virtual environment\npython3.10 -m venv .venv\nsource .venv/bin/activate\n\n# (recommended) Upgrade pip\npython -m pip install --upgrade pip\n\n# Install runtime dependencies\npip install -r requirements.txt\n</code></pre> <p>If the main PyPI index cannot resolve the exact PyTorch build, install the wheel directly from the official index before <code>pip install -r requirements.txt</code>:</p> <pre><code>pip install torch==1.13.1 torchvision==0.14.1 --index-url https://download.pytorch.org/whl/cu117\n</code></pre>"},{"location":"getting-started/#configure-your-experiment","title":"Configure your experiment","text":"<p>All experiment settings live in YAML files under <code>configs/</code>. The <code>config_20m.yaml</code> template targets Sentinel-2 20\u202fm inputs, while <code>config_10m.yaml</code> demonstrates 10\u202fm multi-band training. Each section controls loaders, models, training schedules, and logging behaviour.</p> <pre><code>\n</code></pre> <p>Duplicate a config file if you need to adjust parameters without touching the defaults:</p> <pre><code>cp configs/config_20m.yaml configs/my_experiment.yaml\n</code></pre>"},{"location":"getting-started/#launch-training","title":"Launch training","text":"<p>Use the training script with the desired YAML file:</p> <pre><code>python train.py --config configs/my_experiment.yaml\n</code></pre> <p>Under the hood the script loads the YAML, constructs datasets, wires loggers and callbacks, and launches GPU-accelerated training with Lightning.</p> <pre><code>\n</code></pre> <p>Training logs, checkpoints, and exported validation panels are written into <code>logs/</code> alongside the W&amp;B run.</p>"},{"location":"getting-started/#resume-or-fine-tune","title":"Resume or fine-tune","text":"<p>Two checkpoint switches let you reuse trained weights without editing Python code:</p>"},{"location":"getting-started/#inference-quick-peek","title":"Inference quick peek","text":"<p>For offline inference, instantiate <code>SRGAN_model</code> and call <code>predict_step</code> with low-resolution tensors. The method auto-normalises Sentinel-2 style inputs, runs the generator, histogram matches the output, and denormalises back to the original range.</p> <pre><code>\n</code></pre> <pre><code>from model.SRGAN import SRGAN_model\nmodel = SRGAN_model(config_file_path=\"configs/my_experiment.yaml\")\nmodel.eval()\n\nwith torch.no_grad():\n    sr = model.predict_step(lr_tensor)\n</code></pre>"},{"location":"training/","title":"Training Workflow","text":"<p>Remote-Sensing-SRGAN uses PyTorch Lightning to manage training loops, logging, and checkpointing. This guide explains the runtime workflow, callbacks, and practical tips for stable training on large remote-sensing datasets.</p>"},{"location":"training/#end-to-end-flow","title":"End-to-end flow","text":"<ol> <li>Configuration load: <code>train.py</code> reads the YAML file supplied via <code>--config</code> and passes it to <code>SRGAN_model</code>.</li> <li>Dataset selection: <code>select_dataset</code> constructs train/validation datasets and wraps them into a Lightning <code>DataModule</code>. Dataset statistics are printed for sanity.</li> <li>Logger setup: Weights &amp; Biases, TensorBoard, and a learning-rate monitor are initialised. Checkpoints are saved under <code>logs/&lt;project&gt;/&lt;timestamp&gt;/</code>.</li> <li>Training loop: <code>Trainer.fit</code> launches GPU-accelerated training with callbacks for checkpointing, early stopping, and LR monitoring.</li> </ol> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"training/#lightning-hooks-inside-srgan_model","title":"Lightning hooks inside <code>SRGAN_model</code>","text":"<ul> <li><code>training_step</code>: Receives both generator and discriminator optimisers (Lightning\u2019s GAN pattern). Handles generator-only pretraining, adversarial ramp-up, and logging of scalar losses/metrics.</li> <li><code>validation_step</code>: Generates super-resolved outputs for qualitative logging, computes metrics (PSNR, SSIM, LPIPS if configured), and stores them for epoch-level aggregation.</li> <li><code>configure_optimizers</code>: Creates two Adam optimisers plus <code>ReduceLROnPlateau</code> schedulers based on config values.</li> <li><code>on_validation_epoch_end</code>: Uses <code>utils.logging_helpers.plot_tensors</code> to push LR/SR/HR panels to TensorBoard and Weights &amp; Biases.</li> </ul> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>Inspect <code>model/SRGAN.py</code> for detailed comments describing each step of the training loop and logging behaviour.</p>"},{"location":"training/#callbacks-and-logging","title":"Callbacks and logging","text":"<p><code>train.py</code> wires in several Lightning callbacks out of the box:</p> Callback Purpose <code>ModelCheckpoint</code> Saves <code>last.ckpt</code> and top-2 checkpoints based on <code>Schedulers.metric</code> (default: <code>val_metrics/l1</code>). <code>LearningRateMonitor</code> Logs generator/discriminator learning rates each epoch to W&amp;B/TensorBoard. <code>EarlyStopping</code> Monitors the same validation metric with large patience (250 epochs) to guard against divergence. <pre><code>\n</code></pre> <p>Weights &amp; Biases is configured with <code>project=\"SRGAN_6bands\"</code> and entity <code>opensr</code>. Set the <code>WANDB_PROJECT</code> or edit the script if you need per-experiment projects. TensorBoard logs are stored in <code>logs/</code> alongside W&amp;B run data.</p>"},{"location":"training/#adversarial-training-schedule","title":"Adversarial training schedule","text":"<p>GAN training is stabilised through three mechanisms:</p> <ul> <li>Generator pretraining: For the first <code>Training.g_pretrain_steps</code>, only the generator optimiser runs; the discriminator is skipped. This lets the generator learn a strong content prior before adversarial updates start.</li> <li>Adversarial ramp-up: After pretraining, the adversarial loss weight increases linearly or sigmoidally over <code>Training.adv_loss_ramp_steps</code> until it reaches <code>Losses.adv_loss_beta</code>.</li> <li>Label smoothing: When <code>Training.label_smoothing=True</code>, real labels are reduced to 0.9 to prevent discriminator overconfidence.</li> </ul> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>These heuristics reduce mode collapse and stabilise training on multi-spectral inputs where illumination and texture vary drastically between bands.</p>"},{"location":"training/#practical-tips","title":"Practical tips","text":"<ul> <li>Monitor PSNR/SSIM/LPIPS in W&amp;B to detect spectral artefacts early. If LPIPS diverges while PSNR improves, consider increasing <code>Losses.perceptual_weight</code>.</li> <li>Use moderate <code>train_batch_size</code> values (8\u201316) to balance GPU utilisation and dataset variety. Increase <code>Data.prefetch_factor</code> when <code>num_workers &gt; 0</code> to keep GPUs busy.</li> </ul> <p><pre><code>\n</code></pre> * To benchmark architectures quickly, reuse trained generators by setting <code>Model.load_checkpoint</code> and adjusting only the discriminator or loss weights. * Keep an eye on GPU memory when scaling <code>Generator.n_blocks</code> and <code>n_channels</code>; RRDB and LKA variants are heavier than SRResNet.</p>"}]}