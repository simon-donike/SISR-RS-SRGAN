{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Remote-Sensing SRGAN","text":"<p>Remote-Sensing-SRGAN is a research-grade training stack for single-image super-resolution (SISR) of multispectral satellite imagery. It wraps a flexible generator/discriminator design, configurable loss suite, and remote-sensing specific data pipelines behind a configuration-first workflow. The implementation is optimised for Sentinel-2 but can be adapted to other sensors that provide paired low-/high-resolution observations.</p> <p>Why another SRGAN?</p> <p>Training GANs on multi-band Earth observation data is notoriously brittle. This project codifies the training heuristics that have proven stable in production \u2014 generator pretraining, adversarial weight ramp-up, and configurable discriminator cadence \u2014 while exposing every knob through YAML. The goal is to make it easy to reproduce remote-sensing SR experiments without rewriting boilerplate.</p>"},{"location":"#project-highlights","title":"Project highlights","text":"<ul> <li>Flexible generator zoo. Choose between SRResNet, residual, RCAB, RRDB, large-kernel attention, or conditional GAN backbones with scale factors from 2\u00d7\u20138\u00d7.\u3010F:model/SRGAN.py\u2020L59-L101\u3011</li> <li>Pluggable losses. Combine pixel, spectral, perceptual, adversarial, and total-variation terms with independent weights and activation schedules.\u3010F:model/SRGAN.py\u2020L44-L58\u3011\u3010F:configs/config_10m.yaml\u2020L35-L70\u3011</li> <li>Remote-sensing ready datasets. Sentinel-2 SAFE windowing and the SEN2NAIP worldwide pairs are built in, with Lightning datamodules created on the fly from the config.\u3010F:data/data_utils.py\u2020L1-L95\u3011</li> <li>Stabilised training flow. Generator-only warm-up, adversarial ramp-up, discriminator scheduling, and Lightning callbacks are wired into the training script.\u3010F:train.py\u2020L19-L93\u3011\u3010F:model/SRGAN.py\u2020L34-L43\u3011</li> <li>Comprehensive logging. TensorBoard visualisations, Weights &amp; Biases tracking, and qualitative inspection panels are emitted during training.\u3010F:train.py\u2020L59-L93\u3011\u3010F:model/SRGAN.py\u2020L12-L16\u3011</li> </ul>"},{"location":"#repository-layout","title":"Repository layout","text":"<pre><code>SISR-RS-SRGAN/\n\u251c\u2500\u2500 configs/              # YAML experiment definitions\n\u251c\u2500\u2500 data/                 # Dataset implementations and helpers\n\u251c\u2500\u2500 model/                # LightningModule, generators, discriminators, losses\n\u251c\u2500\u2500 utils/                # Logging and spectral utilities\n\u251c\u2500\u2500 train.py              # Training entry point\n\u2514\u2500\u2500 docs/                 # MkDocs site (you are here)\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Head to Getting Started for environment setup and the minimal training command.</li> <li>Review the Configuration Guide to understand every YAML switch.</li> <li>Dive into Model Components for generator and discriminator details.</li> <li>Explore Data Pipelines for dataset specifics and extension tips.</li> </ul>"},{"location":"architecture/","title":"Model Components","text":"<p>Remote-Sensing-SRGAN builds on a modular Lightning implementation that lets you swap generator and discriminator backbones as well as plug in new loss terms. This section describes the main components and how they interact during training.</p>"},{"location":"architecture/#lightning-module","title":"Lightning module","text":"<p>The central <code>SRGAN_model</code> class orchestrates the entire training loop:</p> <ul> <li>Loads configuration values from YAML via OmegaConf during initialisation.\u3010F:model/SRGAN.py\u2020L25-L43\u3011</li> <li>Instantiates the chosen generator, discriminator, and perceptual loss network through <code>get_models()</code>.\u3010F:model/SRGAN.py\u2020L59-L150\u3011</li> <li>Exposes <code>forward</code> for generator inference and <code>predict_step</code> for deployment-ready predictions with auto-normalisation and histogram matching.\u3010F:model/SRGAN.py\u2020L153-L192\u3011</li> <li>Implements Lightning hooks for adversarial training, including generator-only pretraining, adversarial ramp-up, and rich logging (see <code>training_step</code> and helpers).\u3010F:model/SRGAN.py\u2020L195-L320\u3011</li> </ul>"},{"location":"architecture/#generator-zoo","title":"Generator zoo","text":"<p>Pick the generator backbone by setting <code>Generator.model_type</code> in the config. The options map to classes under <code>model/generators/</code>:</p> Type Description <code>SRResNet</code> Classic SRResNet with residual blocks sans batch norm; a strong baseline for content pretraining.\u3010F:model/SRGAN.py\u2020L64-L76\u3011 <code>res</code> Flexible residual blocks with configurable depth/width defined in <code>FlexibleGenerator</code> (supports residual scaling).\u3010F:model/SRGAN.py\u2020L77-L101\u3011\u3010F:model/generators/flexible_generator.py\u2020L1-L96\u3011 <code>rcab</code> Residual channel attention blocks for finer texture modelling using the same flexible generator with RCAB building blocks.\u3010F:model/generators/flexible_generator.py\u2020L1-L96\u3011\u3010F:model/model_blocks/init.py\u2020L144-L173\u3011 <code>rrdb</code> Residual-in-residual dense blocks (ESRGAN-style) for deeper receptive fields, enabled through the flexible generator registry.\u3010F:model/generators/flexible_generator.py\u2020L21-L96\u3011\u3010F:model/model_blocks/init.py\u2020L176-L213\u3011 <code>lka</code> Large kernel attention variant approximating global receptive fields, useful for broad RS structures.\u3010F:model/generators/flexible_generator.py\u2020L21-L96\u3011\u3010F:model/model_blocks/init.py\u2020L215-L249\u3011 <code>conditional_cgan</code> / <code>cgan</code> Conditional GAN generator that injects latent noise and conditional embeddings in addition to the spectral input.\u3010F:model/SRGAN.py\u2020L88-L101\u3011\u3010F:model/generators/cgan_generator.py\u2020L15-L137\u3011 <p>Each generator consumes <code>Model.in_bands</code> channels, expands to <code>Generator.n_channels</code> features, and upsamples by <code>Generator.scaling_factor</code> using pixel shuffle stages. Kernel sizes (<code>large_kernel_size</code>, <code>small_kernel_size</code>) tailor the receptive field for remote-sensing textures.\u3010F:model/SRGAN.py\u2020L64-L101\u3011\u3010F:model/generators/flexible_generator.py\u2020L49-L96\u3011</p>"},{"location":"architecture/#discriminators","title":"Discriminators","text":"<p>Discriminator selection lives under <code>Discriminator.model_type</code>:</p> <ul> <li><code>standard</code>: The canonical SRGAN discriminator with adjustable depth (<code>n_blocks</code>), implemented in <code>model/descriminators/srgan_discriminator.py</code>. The constructor accepts <code>in_channels</code> (matching the number of input bands).\u3010F:model/SRGAN.py\u2020L102-L117\u3011</li> <li><code>patchgan</code>: Patch-level discriminator inspired by pix2pix, parameterised by <code>n_layers</code> and automatically set from <code>n_blocks</code>. Good for local fidelity supervision on large tiles.\u3010F:model/SRGAN.py\u2020L118-L150\u3011</li> </ul> <p>Because both discriminators receive multi-band inputs, the repo avoids hard-coding RGB assumptions and allows training on arbitrary spectral stacks.</p>"},{"location":"architecture/#losses","title":"Losses","text":"<p>Generator training minimises a combination of content and adversarial losses:</p> <ul> <li>Content loss (<code>GeneratorContentLoss</code>) combines weighted L1, spectral angle mapper (SAM), perceptual (VGG or LPIPS), and total variation terms as configured in the YAML file.\u3010F:model/SRGAN.py\u2020L34-L58\u3011\u3010F:model/loss/loss.py\u2020L1-L210\u3011</li> <li>Adversarial loss uses <code>torch.nn.BCEWithLogitsLoss</code> against discriminator logits with optional label smoothing to improve stability.\u3010F:model/SRGAN.py\u2020L34-L58\u3011</li> </ul> <p>During generator-only pretraining, the adversarial term is disabled; it ramps up to <code>adv_loss_beta</code> over <code>adv_loss_ramp_steps</code> iterations according to the chosen schedule (<code>linear</code> or <code>sigmoid</code>).\u3010F:model/SRGAN.py\u2020L34-L58\u3011\u3010F:configs/config_10m.yaml\u2020L35-L70\u3011</p>"},{"location":"architecture/#normalisation-and-inference-helpers","title":"Normalisation and inference helpers","text":"<p>Remote-sensing imagery often arrives as 0\u201310,000 scaled reflectance. <code>utils.spectral_helpers</code> provides <code>normalise_10k</code> for scaling to 0\u20131 and <code>histogram</code> for histogram matching. <code>predict_step</code> calls both utilities to ensure outputs match the statistical distribution of inputs before returning CPU tensors.\u3010F:model/SRGAN.py\u2020L160-L192\u3011\u3010F:utils/spectral_helpers.py\u2020L53-L200\u3011</p>"},{"location":"architecture/#logging-utilities","title":"Logging utilities","text":"<p>Qualitative logging is handled via <code>utils.logging_helpers.plot_tensors</code>, which renders low-/super-/high-resolution panels for TensorBoard and Weights &amp; Biases. The Lightning module invokes these helpers inside validation hooks so you can monitor spectral fidelity and artefacts during training.\u3010F:model/SRGAN.py\u2020L12-L16\u3011\u3010F:utils/logging_helpers.py\u2020L1-L200\u3011</p>"},{"location":"architecture/#extending-the-model","title":"Extending the model","text":"<ul> <li>Add new generator families by creating a file under <code>model/generators/</code> and wiring it into <code>SRGAN_model.get_models</code> plus <code>model/generators/__init__.py</code>.</li> <li>Introduce alternative discriminators under <code>model/descriminators/</code> and add a new branch in <code>get_models</code>.</li> <li>Extend content loss by editing <code>model/loss/loss.py</code>; the <code>GeneratorContentLoss</code> class already parses weights from the config.</li> <li>For additional logging (e.g., metrics beyond PSNR/SSIM/LPIPS), modify the validation hooks inside <code>model/SRGAN.py</code>.</li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>Remote-Sensing-SRGAN is intentionally configuration-first. Every YAML file under <code>configs/</code> controls the entire experiment lifecycle \u2014 from dataset selection to optimiser schedules. This page documents the key sections and explains how they map to the Python implementation.</p>"},{"location":"configuration/#anatomy-of-a-config-file","title":"Anatomy of a config file","text":"<p>Each config follows the same structure as <code>configs/config_10m.yaml</code>:</p> <pre><code>Data:\n  train_batch_size: 12\n  val_batch_size: 8\n  num_workers: 6\n  prefetch_factor: 2\n  dataset_type: \"SISR_WW\"\n\nModel:\n  in_bands: 6\n  continue_training: False\n  load_checkpoint: False\n\nTraining:\n  pretrain_g_only: True\n  g_pretrain_steps: 15000\n  adv_loss_ramp_steps: 5000\n  label_smoothing: True\n  Losses:\n    adv_loss_beta: 1e-3\n    adv_loss_schedule: sigmoid\n    l1_weight: 1.0\n    sam_weight: 0.05\n    perceptual_weight: 0.1\n    perceptual_metric: vgg\n    tv_weight: 0.0\n    max_val: 1.0\n    ssim_win: 11\n\nGenerator:\n  model_type: cgan\n  large_kernel_size: 9\n  small_kernel_size: 3\n  n_channels: 96\n  n_blocks: 32\n  scaling_factor: 8\n\nDiscriminator:\n  model_type: standard\n  n_blocks: 8\n\nOptimizers:\n  optim_g_lr: 1e-4\n  optim_d_lr: 1e-4\n\nSchedulers:\n  metric: val_metrics/l1\n  patience_g: 100\n  patience_d: 100\n  factor_g: 0.5\n  factor_d: 0.5\n  verbose: True\n\nLogging:\n  num_val_images: 5\n</code></pre>"},{"location":"configuration/#data-block","title":"Data block","text":"<p>The data section controls loader throughput and selects the dataset implementation. The <code>dataset_type</code> key is forwarded to <code>data.data_utils.select_dataset</code>, which instantiates the proper dataset class and wraps it into a Lightning <code>DataModule</code> with the requested batch sizes and worker settings.\u3010F:data/data_utils.py\u2020L1-L95\u3011\u3010F:data/data_utils.py\u2020L97-L150\u3011</p>"},{"location":"configuration/#available-dataset-types","title":"Available dataset types","text":"Key Description <code>S2_6b</code> Loads Sentinel-2 SAFE chips with six 20\u202fm bands (B05, B06, B07, B8A, B11, B12). Histogram-matched low-resolution inputs are synthesised on-the-fly. \u3010F:data/data_utils.py\u2020L17-L48\u3011 <code>S2_4b</code> Reuses the SAFE pipeline for four 10\u202fm bands (B05, B04, B03, B02). Useful for RGB+NIR 4\u00d7 tasks. \u3010F:data/data_utils.py\u2020L50-L82\u3011 <code>SISR_WW</code> Wraps the SEN2NAIP worldwide dataset for 4\u00d7 cross-sensor training. Training/validation splits are constructed from a root directory. \u3010F:data/data_utils.py\u2020L84-L95\u3011 <p>If you introduce a new dataset class, register it by adding another branch to <code>select_dataset</code> and exposing a new <code>dataset_type</code> value.</p>"},{"location":"configuration/#model-block","title":"Model block","text":"<p>The <code>Model</code> section captures properties that affect both training and checkpoint management:</p> <ul> <li><code>in_bands</code>: number of channels consumed by the generator and discriminator. This value is forwarded directly when models are instantiated.\u3010F:model/SRGAN.py\u2020L62-L100\u3011</li> <li><code>load_checkpoint</code>: path to a Lightning checkpoint whose weights should initialise the model before training begins.\u3010F:train.py\u2020L34-L47\u3011</li> <li><code>continue_training</code>: checkpoint to resume including optimiser states and scheduler counters (mapped to <code>Trainer(resume_from_checkpoint=...)</code>).\u3010F:train.py\u2020L34-L48\u3011</li> </ul>"},{"location":"configuration/#training-block","title":"Training block","text":"<p>Training-related switches are consumed inside the Lightning module:</p> <ul> <li><code>pretrain_g_only</code>: number of initial steps where only generator updates run (adversarial term disabled).\u3010F:model/SRGAN.py\u2020L34-L43\u3011</li> <li><code>g_pretrain_steps</code>: length of the generator-only warm-up window.\u3010F:model/SRGAN.py\u2020L34-L43\u3011</li> <li><code>adv_loss_ramp_steps</code>: number of iterations over which the adversarial loss weight ramps to its target. The schedule shape is controlled by <code>Losses.adv_loss_schedule</code>.\u3010F:model/SRGAN.py\u2020L34-L43\u3011\u3010F:configs/config_10m.yaml\u2020L35-L70\u3011</li> <li><code>label_smoothing</code>: enables 0.9 real labels in the discriminator to reduce overconfidence.\u3010F:model/SRGAN.py\u2020L34-L43\u3011</li> </ul> <p>The nested <code>Losses</code> dictionary is passed to <code>GeneratorContentLoss</code>, which mixes pixel-space (<code>l1_weight</code>), spectral (<code>sam_weight</code>), perceptual (<code>perceptual_weight</code> with <code>perceptual_metric</code>), and total-variation (<code>tv_weight</code>) losses. The final adversarial weight after ramp-up is <code>adv_loss_beta</code>. \u3010F:model/SRGAN.py\u2020L44-L58\u3011\u3010F:configs/config_10m.yaml\u2020L35-L70\u3011</p>"},{"location":"configuration/#generator-and-discriminator-blocks","title":"Generator and Discriminator blocks","text":"<p>Generator parameters control the backbone constructed in <code>SRGAN_model.get_models</code>:</p> <ul> <li><code>model_type</code>: choose among <code>SRResNet</code>, <code>res</code>, <code>rcab</code>, <code>rrdb</code>, <code>lka</code>, or conditional GAN variants (<code>conditional_cgan</code>/<code>cgan</code>). Each path maps to a dedicated class under <code>model/generators/</code>.\u3010F:model/SRGAN.py\u2020L59-L101\u3011</li> <li><code>n_channels</code>, <code>n_blocks</code>: width and depth of the residual trunk. These values are forwarded verbatim to the generator constructors.\u3010F:model/SRGAN.py\u2020L64-L101\u3011</li> <li><code>scaling_factor</code>: upsampling factor (2\u00d7, 4\u00d7, or 8\u00d7). Passed into the generator to configure pixel-shuffle stages.\u3010F:model/SRGAN.py\u2020L64-L101\u3011</li> <li><code>large_kernel_size</code>, <code>small_kernel_size</code>: head/tail and residual block kernel sizes to shape receptive field.\u3010F:model/SRGAN.py\u2020L64-L101\u3011</li> </ul> <p>The discriminator section selects either the classic SRGAN CNN (<code>standard</code>) or a PatchGAN variant and optionally specifies convolutional depth via <code>n_blocks</code>. \u3010F:model/SRGAN.py\u2020L102-L130\u3011</p>"},{"location":"configuration/#optimisers-and-schedulers","title":"Optimisers and schedulers","text":"<p>Both the generator and discriminator use Adam optimisers with learning rates read from <code>Optimizers.optim_g_lr</code> and <code>Optimizers.optim_d_lr</code>. The Lightning module instantiates <code>ReduceLROnPlateau</code> schedulers using the parameters provided under <code>Schedulers</code> (<code>metric</code>, <code>patience_*</code>, <code>factor_*</code>, <code>verbose</code>).\u3010F:model/SRGAN.py\u2020L5-L12\u3011\u3010F:configs/config_10m.yaml\u2020L103-L132\u3011</p>"},{"location":"configuration/#logging","title":"Logging","text":"<p>The <code>Logging</code> section controls qualitative outputs. During validation the model logs <code>num_val_images</code> panels via TensorBoard and Weights &amp; Biases. The training script also wires in Weights &amp; Biases, TensorBoard, and learning-rate monitors; edit this block if you want fewer images per epoch. \u3010F:configs/config_10m.yaml\u2020L128-L132\u3011\u3010F:train.py\u2020L59-L93\u3011</p>"},{"location":"configuration/#tips-for-custom-configs","title":"Tips for custom configs","text":"<ul> <li>Keep configs under version control alongside experiment results \u2014 MkDocs can render them for quick reference.</li> <li>Use OmegaConf variable interpolation if you need to reuse values across sections.</li> <li>When experimenting with new datasets, add dataset-specific parameters (paths, band lists) under <code>Data</code> and consume them inside your dataset branch.</li> <li>Prefer creating new YAML files rather than editing defaults; the training script accepts any path via <code>--config</code>.</li> </ul>"},{"location":"data/","title":"Data Pipelines","text":"<p>Remote-Sensing-SRGAN ships with dataset loaders tailored to Sentinel-2 SAFE products and the SEN2NAIP worldwide corpus. All loaders produce <code>(lr, hr)</code> tensor pairs suitable for Lightning training and plug into the config-driven selector in <code>data/data_utils.py</code>.</p>"},{"location":"data/#dataset-selector","title":"Dataset selector","text":"<p><code>data.data_utils.select_dataset(config)</code> reads <code>config.Data.dataset_type</code> and constructs the appropriate dataset instances. The helper then wraps them into a minimal Lightning <code>DataModule</code> with configurable batch sizes, worker counts, and prefetch factors.\u3010F:data/data_utils.py\u2020L1-L150\u3011</p> <pre><code>pl_datamodule = select_dataset(config)\ntrain_loader = pl_datamodule.train_dataloader()\nval_loader = pl_datamodule.val_dataloader()\n</code></pre> <p>The selector currently supports three dataset families:</p>"},{"location":"data/#sentinel-2-safe-6-band-s2_6b","title":"Sentinel-2 SAFE 6-band (<code>S2_6b</code>)","text":"<ul> <li>Goal: Train on six 20\u202fm Sentinel-2 bands (B05, B06, B07, B8A, B11, B12) stacked into a single tensor.</li> <li>Implementation: Uses <code>data/SEN2_SAFE/S2_6b_ds.py::S2SAFEDataset</code> to read a manifest of pre-windowed chips. The dataset handles normalisation, band ordering, LR synthesis via anti-aliased downsampling, and invalid chip filtering.\u3010F:data/data_utils.py\u2020L17-L48\u3011</li> <li>Configuration hooks: <code>Generator.scaling_factor</code> determines the SR scale (2\u00d7/4\u00d7/8\u00d7). Hard-coded manifest path and band order can be promoted to config keys if you need to vary them frequently.</li> </ul>"},{"location":"data/#sentinel-2-safe-4-band-s2_4b","title":"Sentinel-2 SAFE 4-band (<code>S2_4b</code>)","text":"<ul> <li>Goal: Focus on RGB+NIR super-resolution using four 10\u202fm bands (B05, B04, B03, B02).</li> <li>Implementation: Reuses <code>S2SAFEDataset</code> with an alternative band list. Despite the 10\u202fm intent, the manifest path currently points to the 20\u202fm JSON; adjust it if you curate a dedicated 10\u202fm manifest.\u3010F:data/data_utils.py\u2020L50-L82\u3011</li> <li>Configuration hooks: Same as <code>S2_6b</code>; update band order and manifest in the dataset branch if you require different inputs.</li> </ul>"},{"location":"data/#sen2naip-worldwide-sisr_ww","title":"SEN2NAIP Worldwide (<code>SISR_WW</code>)","text":"<ul> <li>Goal: Leverage Taco Foundation\u2019s SEN2NAIP pairs where Sentinel-2 observations are paired with NAIP aerial imagery at 4\u00d7 resolution.</li> <li>Implementation: The <code>SISRWorldWide</code> dataset (under <code>data/SISR_WW/</code>) reads the dataset root, honours <code>split</code> arguments (<code>train</code>/<code>val</code>), and returns aligned <code>(lr, hr)</code> samples. The data selector simply instantiates train/val splits pointing to <code>/data3/SEN2NAIP_global</code> by default.\u3010F:data/data_utils.py\u2020L84-L95\u3011</li> <li>Configuration hooks: Update the root path or expose it through the config to point at your local dataset copy.</li> </ul>"},{"location":"data/#building-new-datasets","title":"Building new datasets","text":"<p>Adding a dataset follows three simple steps:</p> <ol> <li>Implement the dataset class under <code>data/&lt;name&gt;/</code> with <code>__len__</code> and <code>__getitem__</code> returning <code>(lr, hr)</code> arrays or tensors.</li> <li>Register the selector by adding a new <code>elif</code> branch in <code>select_dataset</code> that instantiates your dataset and passes the config values you need.</li> <li>Expose configuration keys under <code>Data</code> (e.g., <code>manifest_path</code>, <code>bands_keep</code>) so experiments remain reproducible without code edits.\u3010F:data/data_utils.py\u2020L1-L95\u3011</li> </ol>"},{"location":"data/#dataloader-configuration","title":"DataLoader configuration","text":"<p><code>datamodule_from_datasets</code> handles DataLoader construction and mirrors config values to PyTorch Lightning:</p> <ul> <li><code>train_batch_size</code> / <code>val_batch_size</code>: separate sizes for each split, falling back to <code>Data.batch_size</code> if provided.\u3010F:data/data_utils.py\u2020L107-L130\u3011</li> <li><code>num_workers</code>: enables multi-process loading; when set to zero the helper disables persistent workers and prefetch factors.\u3010F:data/data_utils.py\u2020L108-L140\u3011</li> <li><code>prefetch_factor</code>: forwarded when workers &gt; 0 to balance host-device throughput.\u3010F:data/data_utils.py\u2020L110-L140\u3011</li> <li><code>shuffle</code>: enabled for both train and validation to improve spatial diversity in evaluation batches (intentional design choice).\u3010F:data/data_utils.py\u2020L125-L140\u3011</li> </ul> <p>Because the <code>DataModule</code> is assembled dynamically, you can plug in custom datasets without changing the training loop \u2014 just add a branch and update your YAML.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks through environment setup, configuration management, and the minimal commands required to launch a Remote-Sensing-SRGAN experiment.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 (required by the pinned PyTorch 1.13 / torchvision 0.14 wheels).\u3010F:README.md\u2020L45-L73\u3011</li> <li>CUDA-capable GPU if you plan to train on large Sentinel-2 chips (the default trainer is configured for GPU execution).\u3010F:train.py\u2020L69-L90\u3011</li> <li>Optional: Weights &amp; Biases account for experiment tracking and Git LFS if you plan to download large datasets.</li> </ul>"},{"location":"getting-started/#create-an-environment","title":"Create an environment","text":"<pre><code># Clone the repository\ngit clone https://github.com/ESAOpenSR/Remote-Sensing-SRGAN.git\ncd Remote-Sensing-SRGAN\n\n# (optional) Create a Python 3.10 virtual environment\npython3.10 -m venv .venv\nsource .venv/bin/activate\n\n# (recommended) Upgrade pip\npython -m pip install --upgrade pip\n\n# Install runtime dependencies\npip install -r requirements.txt\n</code></pre> <p>If the main PyPI index cannot resolve the exact PyTorch build, install the wheel directly from the official index before <code>pip install -r requirements.txt</code>:</p> <pre><code>pip install torch==1.13.1 torchvision==0.14.1 --index-url https://download.pytorch.org/whl/cu117\n</code></pre>"},{"location":"getting-started/#configure-your-experiment","title":"Configure your experiment","text":"<p>All experiment settings live in YAML files under <code>configs/</code>. The <code>config_20m.yaml</code> template targets Sentinel-2 20\u202fm inputs, while <code>config_10m.yaml</code> demonstrates 10\u202fm multi-band training. Key sections include:</p> <ul> <li><code>Data</code>: batch sizes, worker counts, and dataset selector (<code>S2_6b</code>, <code>S2_4b</code>, <code>SISR_WW</code>, etc.).\u3010F:configs/config_10m.yaml\u2020L12-L33\u3011</li> <li><code>Model</code>: input band count and checkpoint loading behaviour.\u3010F:configs/config_10m.yaml\u2020L22-L28\u3011</li> <li><code>Training</code>: warm-up lengths, adversarial ramp strategy, and loss weights.\u3010F:configs/config_10m.yaml\u2020L35-L70\u3011</li> <li><code>Generator</code> / <code>Discriminator</code>: architecture choices and scale factor.\u3010F:configs/config_10m.yaml\u2020L73-L101\u3011</li> <li><code>Optimizers</code>, <code>Schedulers</code>, <code>Logging</code>: learning rates, ReduceLROnPlateau settings, and validation visualisations.\u3010F:configs/config_10m.yaml\u2020L103-L132\u3011</li> </ul> <p>Duplicate a config file if you need to adjust parameters without touching the defaults:</p> <pre><code>cp configs/config_20m.yaml configs/my_experiment.yaml\n</code></pre>"},{"location":"getting-started/#launch-training","title":"Launch training","text":"<p>Use the training script with the desired YAML file:</p> <pre><code>python train.py --config configs/my_experiment.yaml\n</code></pre> <p>Under the hood the script:</p> <ol> <li>Loads the configuration via OmegaConf and instantiates the Lightning <code>SRGAN_model</code>.\u3010F:train.py\u2020L24-L48\u3011</li> <li>Selects and wraps the dataset into a Lightning <code>DataModule</code> based on <code>Data.dataset_type</code>.\u3010F:train.py\u2020L49-L57\u3011\u3010F:data/data_utils.py\u2020L1-L95\u3011</li> <li>Configures Weights &amp; Biases, TensorBoard, and learning rate monitoring callbacks. \u3010F:train.py\u2020L59-L93\u3011</li> <li>Starts training with GPU acceleration enabled by default. \u3010F:train.py\u2020L69-L90\u3011</li> </ol> <p>Training logs, checkpoints, and exported validation panels are written into <code>logs/</code> alongside the W&amp;B run.</p>"},{"location":"getting-started/#resume-or-fine-tune","title":"Resume or fine-tune","text":"<p>Two checkpoint switches let you reuse trained weights without editing Python code:</p> <ul> <li><code>Model.load_checkpoint</code>: path to a Lightning checkpoint whose weights should initialise the generator/discriminator before training begins. \u3010F:train.py\u2020L34-L47\u3011</li> <li><code>Model.continue_training</code>: path to a checkpoint that should be fully resumed (optimizer states, schedulers, etc.). Leave both as <code>False</code> to start from scratch.\u3010F:train.py\u2020L34-L47\u3011</li> </ul>"},{"location":"getting-started/#inference-quick-peek","title":"Inference quick peek","text":"<p>For offline inference, instantiate <code>SRGAN_model</code> and call <code>predict_step</code> with low-resolution tensors. The method auto-normalises Sentinel-2 style inputs, runs the generator, histogram matches the output, and denormalises back to the original range.\u3010F:model/SRGAN.py\u2020L103-L144\u3011</p> <pre><code>from model.SRGAN import SRGAN_model\nmodel = SRGAN_model(config_file_path=\"configs/my_experiment.yaml\")\nmodel.eval()\n\nwith torch.no_grad():\n    sr = model.predict_step(lr_tensor)\n</code></pre>"},{"location":"training/","title":"Training Workflow","text":"<p>Remote-Sensing-SRGAN uses PyTorch Lightning to manage training loops, logging, and checkpointing. This guide explains the runtime workflow, callbacks, and practical tips for stable training on large remote-sensing datasets.</p>"},{"location":"training/#end-to-end-flow","title":"End-to-end flow","text":"<ol> <li>Configuration load: <code>train.py</code> reads the YAML file supplied via <code>--config</code> and passes it to <code>SRGAN_model</code>.\u3010F:train.py\u2020L19-L47\u3011</li> <li>Dataset selection: <code>select_dataset</code> constructs train/validation datasets and wraps them into a Lightning <code>DataModule</code>. Dataset statistics are printed for sanity. \u3010F:data/data_utils.py\u2020L1-L95\u3011\u3010F:data/data_utils.py\u2020L121-L140\u3011</li> <li>Logger setup: Weights &amp; Biases, TensorBoard, and a learning-rate monitor are initialised. Checkpoints are saved under <code>logs/&lt;project&gt;/&lt;timestamp&gt;/</code>.\u3010F:train.py\u2020L59-L93\u3011</li> <li>Training loop: <code>Trainer.fit</code> launches GPU-accelerated training with callbacks for checkpointing, early stopping, and LR monitoring. \u3010F:train.py\u2020L69-L96\u3011</li> </ol>"},{"location":"training/#lightning-hooks-inside-srgan_model","title":"Lightning hooks inside <code>SRGAN_model</code>","text":"<ul> <li><code>training_step</code>: Receives both generator and discriminator optimisers (Lightning\u2019s GAN pattern). Handles generator-only pretraining, adversarial ramp-up, and logging of scalar losses/metrics.</li> <li><code>validation_step</code>: Generates super-resolved outputs for qualitative logging, computes metrics (PSNR, SSIM, LPIPS if configured), and stores them for epoch-level aggregation.</li> <li><code>configure_optimizers</code>: Creates two Adam optimisers plus <code>ReduceLROnPlateau</code> schedulers based on config values. \u3010F:model/SRGAN.py\u2020L5-L12\u3011</li> <li><code>on_validation_epoch_end</code>: Uses <code>utils.logging_helpers.plot_tensors</code> to push LR/SR/HR panels to TensorBoard and Weights &amp; Biases. \u3010F:model/SRGAN.py\u2020L12-L16\u3011\u3010F:utils/logging_helpers.py\u2020L1-L200\u3011</li> </ul> <p>Inspect <code>model/SRGAN.py</code> for detailed comments describing each step of the training loop and logging behaviour.</p>"},{"location":"training/#callbacks-and-logging","title":"Callbacks and logging","text":"<p><code>train.py</code> wires in several Lightning callbacks out of the box:</p> Callback Purpose <code>ModelCheckpoint</code> Saves <code>last.ckpt</code> and top-2 checkpoints based on <code>Schedulers.metric</code> (default: <code>val_metrics/l1</code>).\u3010F:train.py\u2020L69-L93\u3011 <code>LearningRateMonitor</code> Logs generator/discriminator learning rates each epoch to W&amp;B/TensorBoard.\u3010F:train.py\u2020L88-L93\u3011 <code>EarlyStopping</code> Monitors the same validation metric with large patience (250 epochs) to guard against divergence. \u3010F:train.py\u2020L80-L88\u3011 <p>Weights &amp; Biases is configured with <code>project=\"SRGAN_6bands\"</code> and entity <code>opensr</code>. Set the <code>WANDB_PROJECT</code> or edit the script if you need per-experiment projects. TensorBoard logs are stored in <code>logs/</code> alongside W&amp;B run data.\u3010F:train.py\u2020L59-L93\u3011</p>"},{"location":"training/#adversarial-training-schedule","title":"Adversarial training schedule","text":"<p>GAN training is stabilised through three mechanisms:</p> <ul> <li>Generator pretraining: For the first <code>Training.g_pretrain_steps</code>, only the generator optimiser runs; the discriminator is skipped. This lets the generator learn a strong content prior before adversarial updates start.\u3010F:model/SRGAN.py\u2020L34-L43\u3011\u3010F:configs/config_10m.yaml\u2020L35-L52\u3011</li> <li>Adversarial ramp-up: After pretraining, the adversarial loss weight increases linearly or sigmoidally over <code>Training.adv_loss_ramp_steps</code> until it reaches <code>Losses.adv_loss_beta</code>.\u3010F:model/SRGAN.py\u2020L34-L43\u3011\u3010F:configs/config_10m.yaml\u2020L35-L70\u3011</li> <li>Label smoothing: When <code>Training.label_smoothing=True</code>, real labels are reduced to 0.9 to prevent discriminator overconfidence.\u3010F:model/SRGAN.py\u2020L34-L43\u3011</li> </ul> <p>These heuristics reduce mode collapse and stabilise training on multi-spectral inputs where illumination and texture vary drastically between bands.</p>"},{"location":"training/#practical-tips","title":"Practical tips","text":"<ul> <li>Monitor PSNR/SSIM/LPIPS in W&amp;B to detect spectral artefacts early. If LPIPS diverges while PSNR improves, consider increasing <code>Losses.perceptual_weight</code>.</li> <li>Use moderate <code>train_batch_size</code> values (8\u201316) to balance GPU utilisation and dataset variety. Increase <code>Data.prefetch_factor</code> when <code>num_workers &gt; 0</code> to keep GPUs busy.\u3010F:data/data_utils.py\u2020L97-L140\u3011</li> <li>To benchmark architectures quickly, reuse trained generators by setting <code>Model.load_checkpoint</code> and adjusting only the discriminator or loss weights.</li> <li>Keep an eye on GPU memory when scaling <code>Generator.n_blocks</code> and <code>n_channels</code>; RRDB and LKA variants are heavier than SRResNet.</li> </ul>"}]}